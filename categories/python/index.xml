<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on codechf的个人博客</title>
    <link>/categories/python/</link>
    <description>Recent content in Python on codechf的个人博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-Hans</language>
    <copyright>&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;&lt;/a&gt;本作品采用&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议&lt;/a&gt;进行许可。</copyright>
    <lastBuildDate>Thu, 01 Oct 2020 09:27:39 +0800</lastBuildDate>
    
	<atom:link href="/categories/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Python爬虫问题</title>
      <link>/posts/python%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98/</link>
      <pubDate>Thu, 01 Oct 2020 09:27:39 +0800</pubDate>
      
      <guid>/posts/python%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98/</guid>
      <description>出问题的代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 beautifulSoup_list = [] # 保存建立的各省份BeautifulSoup for item in province_url: req_second = urllib.request.Request(url=item, headers=headers) respoae_second = urllib.request.urlopen(req_second) soup_second = BeautifulSoup(respoae_second, &amp;#39;html.parser&amp;#39;) beautifulSoup_list.append(soup_second) # 获取</description>
    </item>
    
    <item>
      <title>Python爬虫问题</title>
      <link>/posts/python%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98%E7%9A%84%E5%89%AF%E6%9C%AC/</link>
      <pubDate>Thu, 01 Oct 2020 09:27:39 +0800</pubDate>
      
      <guid>/posts/python%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98%E7%9A%84%E5%89%AF%E6%9C%AC/</guid>
      <description>出问题的代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 beautifulSoup_list = [] # 保存建立的各省份BeautifulSoup for item in province_url: req_second = urllib.request.Request(url=item, headers=headers) respoae_second = urllib.request.urlopen(req_second) soup_second = BeautifulSoup(respoae_second, &amp;#39;html.parser&amp;#39;) beautifulSoup_list.append(soup_second) # 获取</description>
    </item>
    
  </channel>
</rss>